{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a42cf1-4128-4d9b-843d-2347ff2f7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3379e72c-b666-40f6-8077-c15307c5b281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sdss-gs.tgz', '.DS_Store', 'spectra', 'extension.ipynb', 'data.csv', 'final.ipynb', 'project_notebook_updated.ipynb', '.gitattributes', 'explore_data_analysis.ipynb', 'sdss-gs', '.ipynb_checkpoints', 'final_copy.ipynb', '.git']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(\".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8bfdc5c-e740-442a-b3eb-54e20ac01eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 objid    mjd  plate   tile  fiberid   run  rerun  camcol  \\\n",
      "0  1237674649924469005  51612    280    108      187  6793    301       3   \n",
      "1  1237652943170371605  52262    743    523       38  1739    301       3   \n",
      "2  1237663787951653251  56326   6374  15286      516  4264    301       3   \n",
      "3  1237658803103531163  56605   6668  15549      640  3103    301       6   \n",
      "4  1237678892819808615  56596   6606  15223      492  7781    301       2   \n",
      "\n",
      "   field         ra  ...  modelMag_i modelMag_z  redshift   stellarmass  \\\n",
      "0     65  170.36352  ...    16.42507   16.06842  0.100474  5.198358e+10   \n",
      "1    227  348.34067  ...    15.30176   14.93129  0.039402  2.520189e+10   \n",
      "2     57  113.34011  ...    18.88552   18.46291  0.278570  8.518087e+10   \n",
      "3     31  153.60405  ...    19.07585   18.92487  0.172246  4.401721e+10   \n",
      "4    134   31.91200  ...    17.29700   16.91050  0.175692  1.768276e+10   \n",
      "\n",
      "    w1mag   w2mag   w3mag  w4mag  gz2c_f  gz2c_s  \n",
      "0  13.423  12.911   9.871  7.613      Sb      Sb  \n",
      "1  12.363  12.207   8.832  6.230     NaN     NaN  \n",
      "2  14.545  14.340  11.529  8.006     NaN     NaN  \n",
      "3  15.948  14.877  11.143  7.712     NaN     NaN  \n",
      "4  14.247  14.049  11.654  8.120     NaN     NaN  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "tgz_path = 'sdss-gs.tgz'\n",
    "\n",
    "csv_path_in_archive = 'sdss-gs/data.csv'\n",
    "\n",
    "with tarfile.open(tgz_path, 'r:gz') as tar:\n",
    "    f = tar.extractfile(csv_path_in_archive)\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1802b0ab-fbad-4dac-8eb9-afd23fe03a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "X_train: (63654, 1000)  | X_test: (15914, 1000)\n"
     ]
    }
   ],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Define a common wavelength grid (example: 1000 points from min to max)\n",
    "all_wavelengths = []\n",
    "for objid in df['objid']:\n",
    "    data = np.loadtxt(f'./spectra/{objid}.csv', delimiter=',', skiprows=1, usecols=(0, 1))\n",
    "    wavelength = data[:, 0]\n",
    "    all_wavelengths.extend(wavelength)\n",
    "common_min, common_max = min(all_wavelengths), max(all_wavelengths)\n",
    "common_grid = np.linspace(common_min, common_max, 1000)\n",
    "\n",
    "X_list_interp = []\n",
    "for objid in df['objid']:\n",
    "    data = np.loadtxt(f'./spectra/{objid}.csv', delimiter=',', skiprows=1, usecols=(0, 1))\n",
    "    wavelength, flux = data[:, 0], data[:, 1]\n",
    "    # Create an interpolation function\n",
    "    interp_func = interp1d(wavelength, flux, bounds_error=False, fill_value=\"extrapolate\")\n",
    "    # Interpolate to the common grid\n",
    "    flux_interp = interp_func(common_grid)\n",
    "    X_list_interp.append(flux_interp)\n",
    "X = np.array(X_list_interp)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['class'].values)  # e.g. Elliptical -> 0, Spiral -> 1\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "print(\"Dataset shapes:\")\n",
    "print(\"X_train:\", X_train.shape, \" | X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "484d3dcd-e521-4c70-b943-bb0442120535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA explained variance (50 components): 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature dimension: 1000\n",
      "Reduced feature dimension: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA, FastICA, NMF\n",
    "\n",
    "# Number of components for dimensionality reduction\n",
    "n_components = 40\n",
    "\n",
    "# 1. Principal Component Analysis (PCA)\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"PCA explained variance (50 components):\",\n",
    "      f\"{100*pca.explained_variance_ratio_.sum():.2f}%\")\n",
    "\n",
    "# 2. Independent Component Analysis (ICA)\n",
    "ica = FastICA(n_components=n_components, random_state=42)\n",
    "X_train_ica = ica.fit_transform(X_train)\n",
    "X_test_ica = ica.transform(X_test)\n",
    "\n",
    "# 3. Non-Negative Matrix Factorization (NMF)\n",
    "# Ensure no negative values (NMF requires non-negative data)\n",
    "X_train_nmf = np.clip(X_train, a_min=0, a_max=None)\n",
    "X_test_nmf = np.clip(X_test, a_min=0, a_max=None)\n",
    "nmf = NMF(n_components=n_components, init='random', random_state=42)\n",
    "X_train_nmf = nmf.fit_transform(X_train_nmf)\n",
    "X_test_nmf = nmf.transform(X_test_nmf)\n",
    "\n",
    "print(\"Original feature dimension:\", X_train.shape[1])\n",
    "print(\"Reduced feature dimension:\", X_train_pca.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f129cae-3b01-4174-9be8-a8a2b0e58cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c310f20c-3a7c-45c9-b34b-5cad8ae083c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# Define a function to create a CNN model for a given input length\n",
    "def make_cnn(input_length, num_classes=2):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=(input_length, 1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare data shapes for CNN (add channel dimension)\n",
    "X_train_full = X_train[..., np.newaxis]   # shape (n_samples, n_wavelengths, 1)\n",
    "X_test_full = X_test[..., np.newaxis,]\n",
    "X_train_pca_cnn = X_train_pca[..., np.newaxis]   # shape (n_samples, 50, 1)\n",
    "X_test_pca_cnn = X_test_pca[..., np.newaxis]\n",
    "X_train_ica_cnn = X_train_ica[..., np.newaxis]\n",
    "X_test_ica_cnn = X_test_ica[..., np.newaxis]\n",
    "X_train_nmf_cnn = X_train_nmf[..., np.newaxis]\n",
    "X_test_nmf_cnn = X_test_nmf[..., np.newaxis]\n",
    "\n",
    "# Build CNN models for each input scenario\n",
    "model_full = make_cnn(input_length=X_train_full.shape[1])\n",
    "model_pca  = make_cnn(input_length=X_train_pca_cnn.shape[1])\n",
    "model_ica  = make_cnn(input_length=X_train_ica_cnn.shape[1])\n",
    "model_nmf  = make_cnn(input_length=X_train_nmf_cnn.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03b32dc4-db9b-434f-9d04-c4ad99f6d356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Full spectra): 1.000\n",
      "Test Accuracy (PCA 50 comps): 1.000\n",
      "Test Accuracy (ICA 50 comps): 1.000\n",
      "Test Accuracy (NMF 50 comps): 1.000\n"
     ]
    }
   ],
   "source": [
    "# Train the CNN on full spectra\n",
    "history_full = model_full.fit(X_train_full, y_train, epochs=20, batch_size=32, \n",
    "                              validation_split=0.1, verbose=0)\n",
    "# Evaluate on test set\n",
    "loss_full, acc_full = model_full.evaluate(X_test_full, y_test, verbose=0)\n",
    "\n",
    "# Train the CNN on PCA-reduced data\n",
    "history_pca = model_pca.fit(X_train_pca_cnn, y_train, epochs=20, batch_size=32, \n",
    "                            validation_split=0.1, verbose=0)\n",
    "loss_pca, acc_pca = model_pca.evaluate(X_test_pca_cnn, y_test, verbose=0)\n",
    "\n",
    "# Train the CNN on ICA-reduced data\n",
    "history_ica = model_ica.fit(X_train_ica_cnn, y_train, epochs=20, batch_size=32, \n",
    "                            validation_split=0.1, verbose=0)\n",
    "loss_ica, acc_ica = model_ica.evaluate(X_test_ica_cnn, y_test, verbose=0)\n",
    "\n",
    "# Train the CNN on NMF-reduced data\n",
    "history_nmf = model_nmf.fit(X_train_nmf_cnn, y_train, epochs=20, batch_size=32, \n",
    "                            validation_split=0.1, verbose=0)\n",
    "loss_nmf, acc_nmf = model_nmf.evaluate(X_test_nmf_cnn, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test Accuracy (Full spectra): {acc_full:.3f}\")\n",
    "print(f\"Test Accuracy (PCA 50 comps): {acc_pca:.3f}\")\n",
    "print(f\"Test Accuracy (ICA 50 comps): {acc_ica:.3f}\")\n",
    "print(f\"Test Accuracy (NMF 50 comps): {acc_nmf:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1c712-1159-4397-9c24-089fc44d74a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
